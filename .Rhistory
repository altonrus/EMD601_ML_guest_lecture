mutate(fetal_health = as.factor(ifelse(fetal_health==1,"Normal", "Abnormal")))
str(dt)
#| echo: true
set.seed(125) #for reproducibility
ctg_split <- initial_split(dt, prop = 0.8)
ctg_split
#Distribution of outcome in train and test sets
table(training(ctg_split)$fetal_health)/nrow(training(ctg_split))
table(testing(ctg_split)$fetal_health)/nrow(testing(ctg_split))
#| echo: true
#Split while stratifying on outcome
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Distribution of outcome in train and test sets
table(training(ctg_split_strat)$fetal_health)/nrow(training(ctg_split_strat))
table(testing(ctg_split_strat)$fetal_health)/nrow(testing(ctg_split_strat))
#| echo: true
ctg_folds <- vfold_cv(training(ctg_split_strat)) # v = 10 is default
ctg_folds$splits[1:3]
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#specify a recipe
rf_tune_recipe <- recipe(fetal_health~., data=dt)
#specify random forest model model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune()) |>
set_mode("classification") |>
set_engine("ranger")
rf_tune_spec
#Create grid of hyperparameters for tuning
rf_grid <- grid_regular(
mtry(range = c(10, 20)),
min_n(range = c(2, 6)),
levels = 3
)
rf_grid
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
rf_tune_recipe,
resamples = ctg_folds,
grid = rf_grid
)
rf_tune_results
rf_tune_results$.metrics
autoplot(rf_tune_results)
theme_set(theme_bw())
#| echo = true
rf_tune_spec |>
fit(fetal_health~., data = dt)
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
rf_tune_recipe,
resamples = ctg_folds,
grid = rf_grid
)
autoplot(rf_tune_results)
show_best(xgb_res, metric = "roc_auc", n = 3)
show_best(rf_tune_results, metric = "roc_auc")
show_best(rf_tune_results, metric = "roc_auc")
autoplot(rf_tune_results)
best_auc <- select_best(rf_tune_results, metric = "roc_auc")
best_auc
?finalize_workflow
?last_fit
rf_test_results <- last_fit(
best_auc,
ctg_recipe,
split = ctg_folds)
ctg_recipe
2*3
6*9
#| echo: true
# install.packages("tidyverse","tidymodels", "ranger")
library(tidyverse)
library(tidymodels)
library(ranger)
theme_set(theme_bw())
#| echo: true
dt <- read_csv("fetal_health.csv") |>
mutate(fetal_health = as.factor(ifelse(fetal_health==1,"Normal", "Abnormal")))
str(dt)
#| echo: true
set.seed(125) #for reproducibility
ctg_split <- initial_split(dt, prop = 0.8)
ctg_split
#Distribution of outcome in train and test sets
table(training(ctg_split)$fetal_health)/nrow(training(ctg_split))
table(testing(ctg_split)$fetal_health)/nrow(testing(ctg_split))
#| echo: true
#Split while stratifying on outcome
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Distribution of outcome in train and test sets
table(training(ctg_split_strat)$fetal_health)/nrow(training(ctg_split_strat))
table(testing(ctg_split_strat)$fetal_health)/nrow(testing(ctg_split_strat))
#| echo: true
ctg_folds <- vfold_cv(training(ctg_split_strat)) # v = 10 is default
ctg_folds$splits[1:3]
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#specify a recipe
ctg_recipe <- recipe(fetal_health~., data=dt)
#specify random forest model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune(),
mode = "classification")
rf_tune_spec
#| echo: true
#Create grid of hyperparameters for tuning
rf_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
#| echo: true
# Tune the model
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = rf_grid
)
#| echo: true
autoplot(rf_tune_results)
en_grid <- expand_grid(
en_grid
en_grid
en_grid <- expand_grid(
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
?expand_grid
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
choose(21, 2)
best_auc <- select_best(rf_tune_results, metric = "roc_auc")
best_auc
rf_test_results <- last_fit(
best_auc,
ctg_recipe,
split = ctg_folds)
?select_best
best_auc
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification")
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_folds)
?last_fit
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_split_strat)
show_best(rf_tune_results, metric = "roc_auc")
rf_test_results
show_best(rf_tune_results, metric="roc_auc", n=3)
rf_test_results
rf_test_results %>% collect_metrics()
predict(extract_workflow(rf_test_results), testing(ctg_split_strat))
?predict
predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
preds <- predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
dt_pres_outcome <- cbind(preds,
testing(ctg_split_strat)$fetal_health)
dt_pres_outcome
head(dt_pres_outcome,5)
dt_pres_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pres_outcome,5)
str(truth)
str(dt_pres_outcome)
#| echo: true
roc_cuve(dt_pres_outcome, truth, .pred_Normal)
#| echo: true
roc_curve(dt_pres_outcome, truth, .pred_Normal)
#| echo: true
roc <- roc_curve(dt_pres_outcome, truth, .pred_Normal)
autoplot(roc)
#| echo: true
roc <- roc_curve(dt_pres_outcome, truth, .pred_Abnormal)
autoplot(roc)
library(caret)
install.packages("caret")
dt_pres_outcome
dt_pred_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pred_outcome,5)
caret::calibration(truth ~ .pred_Normal, data = dt_pred_outcome)
calibration_obj <- caret::calibration(truth ~ .pred_Normal, data = dt_pred_outcome)
ggplot(calibration_obj)
calibration_obj <- caret::calibration(truth ~ .pred_Abormal, data = dt_pred_outcome)
calibration_obj <- caret::calibration(truth ~ .pred_Abnormal, data = dt_pred_outcome)
ggplot(calibration_obj)
calibration_obj <- caret::calibration(truth ~ .pred_Abnormal, data = dt_pred_outcome,
breaks = 6)
ggplot(calibration_obj)
ctg_folds
ctg_folds$splits[1:2]
roc
?roc
auc(roc)
roc_auc(roc)
roc_auc(dt_pred_outcome, truth, .pred_Abnormal)
#| echo: true
rf_test_results %>% collect_metrics()
?ranger
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification".
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification",
importance = "impurity")
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification") |>
set_engine("ranger", importance = "impurity")
#| echo: true
rf_test_results %>%
extract_fit_parsnip() %>%
vip(num_features = 20)
#| echo: true
rf_test_results %>%
extract_fit_parsnip()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
autoplot()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
variableImpPlot()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
importance.ranger()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
importance()
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
importance()
#| echo: true
rf_test_results |>
importance()
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
vip(num_features = 10)
install.packages("vip")
library(vip)
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
vip(num_features = 10)
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification") |>
set_engine("ranger", importance = "impurity")
#Trains top configuration on all training set; predict on test set
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_split_strat)
#| echo: true
#Estiamte unbiased performance on test set
rf_test_results %>% collect_metrics()
# Compare predicted risk to actual outcome
preds <- predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
dt_pred_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pred_outcome,5)
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
vip(num_features = 10)
?calibration
#| echo: true
#Create grid of hyperparameters for tuning
rf_grid <- grid_regular(
mtry(range = c(1, 10)),#number of predictors sampled at each split of tree
min_n(range = c(2, 11)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
?autoplot
#| echo: true
# install.packages("tidyverse","tidymodels", "ranger")
library(tidyverse)
library(tidymodels)
library(ranger) #random forest engine
theme_set(theme_bw())
#| echo: true
dt <- read_csv("fetal_health.csv") |>
mutate(fetal_health = as.factor(ifelse(fetal_health==1,"Normal", "Abnormal")))
str(dt)
#specify a recipe (what are we predicting, what are the features)
ctg_recipe <- recipe(fetal_health~., data=dt)
#| echo: true
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
set.seed(456)
#Split whole data into test and training, stratifying on the outcome
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Create cross validation folds in test set
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#specify random forest model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune(),
mode = "classification")
rf_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
set.seed(456)
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = rf_grid
)
autoplot(rf_tune_results)
show_best(rf_tune_results)
#| echo: true
#Split whole data into test and training, stratifying on the outcome
set.seed(456)
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Create cross validation folds in test set
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#Split whole data into test and training, stratifying on the outcome
set.seed(456)
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Create cross validation folds in test set
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#specify random forest model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune(),
mode = "classification")
rf_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = rf_grid
)
autoplot(rf_tune_results)
show_best(rf_tune_results)
#specify a recipe for all individual variables AND pairwise interaction terms
ctg_recipe_interactions <- recipe(fetal_health~.^2, data=dt)
#specify a recipe (what are we predicting, what are the features)
ctg_recipe <- recipe(fetal_health~., data=dt)
#specify a recipe for all individual variables AND pairwise interaction terms
ctg_recipe_interactions <- ctg_recipe |>
step_interact(terms = ~.:.)
ctg_recipe_interactions
#specify elasticnet model for tuning
en_tune_spec <- logistic_reg(penalty = tune(),
mixture = tune(),
mode = "classification")
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
set.seed(456)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe_interactions,
resamples = ctg_folds,
grid = en_grid
)
#specify elasticnet model for tuning
en_tune_spec <- logistic_reg(penalty = tune(),
mixture = tune(),
mode = "classification",
engine = "glmnet")
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
set.seed(456)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe_interactions,
resamples = ctg_folds,
grid = en_grid
)
library(glmnet)
install.packages("glmnet")
library(glmnet)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe_interactions,
resamples = ctg_folds,
grid = en_grid
)
en_tune_results
autoplot(en_tune_results)
?selections
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = en_grid
)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = en_grid
)
autoplot(en_tune_results)
show_best(en_tune_results)
#specify random forest model for tuning
gbm_tune_spec <- boost_tree(mtry = tune(),
trees = 1000,
min_n = tune(),
tree_depth = tune(),
stop_iter = 3,
mode = "classification")
gbm_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),
tree_depth = c(3, 9),
levels = 3
)
gbm_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),
tree_depth(range = c(3, 9)),
levels = 3
)
gbm_grid
set.seed(456)
gbm_tune_results <- tune_grid(
gbm_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = gbm_grid
)
install.packages("xgboost")
gbm_tune_results <- tune_grid(
gbm_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = gbm_grid
)
autoplot(rf_tune_results)
autoplot(gbm_tune_results)
show_best(gbm_tune_results, metric = "roc_auc")
#| echo: true
show_best(rf_tune_results)
show_best(en_tune_results)
show_best(gbm_tune_results)
2*3*3*3

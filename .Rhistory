en_grid
choose(21, 2)
best_auc <- select_best(rf_tune_results, metric = "roc_auc")
best_auc
rf_test_results <- last_fit(
best_auc,
ctg_recipe,
split = ctg_folds)
?select_best
best_auc
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification")
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_folds)
?last_fit
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_split_strat)
show_best(rf_tune_results, metric = "roc_auc")
rf_test_results
show_best(rf_tune_results, metric="roc_auc", n=3)
rf_test_results
rf_test_results %>% collect_metrics()
predict(extract_workflow(rf_test_results), testing(ctg_split_strat))
?predict
predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
preds <- predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
dt_pres_outcome <- cbind(preds,
testing(ctg_split_strat)$fetal_health)
dt_pres_outcome
head(dt_pres_outcome,5)
dt_pres_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pres_outcome,5)
str(truth)
str(dt_pres_outcome)
#| echo: true
roc_cuve(dt_pres_outcome, truth, .pred_Normal)
#| echo: true
roc_curve(dt_pres_outcome, truth, .pred_Normal)
#| echo: true
roc <- roc_curve(dt_pres_outcome, truth, .pred_Normal)
autoplot(roc)
#| echo: true
roc <- roc_curve(dt_pres_outcome, truth, .pred_Abnormal)
autoplot(roc)
library(caret)
install.packages("caret")
dt_pres_outcome
dt_pred_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pred_outcome,5)
caret::calibration(truth ~ .pred_Normal, data = dt_pred_outcome)
calibration_obj <- caret::calibration(truth ~ .pred_Normal, data = dt_pred_outcome)
ggplot(calibration_obj)
calibration_obj <- caret::calibration(truth ~ .pred_Abormal, data = dt_pred_outcome)
calibration_obj <- caret::calibration(truth ~ .pred_Abnormal, data = dt_pred_outcome)
ggplot(calibration_obj)
calibration_obj <- caret::calibration(truth ~ .pred_Abnormal, data = dt_pred_outcome,
breaks = 6)
ggplot(calibration_obj)
ctg_folds
ctg_folds$splits[1:2]
roc
?roc
auc(roc)
roc_auc(roc)
roc_auc(dt_pred_outcome, truth, .pred_Abnormal)
#| echo: true
rf_test_results %>% collect_metrics()
?ranger
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification".
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification",
importance = "impurity")
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification") |>
set_engine("ranger", importance = "impurity")
#| echo: true
rf_test_results %>%
extract_fit_parsnip() %>%
vip(num_features = 20)
#| echo: true
rf_test_results %>%
extract_fit_parsnip()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
autoplot()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
variableImpPlot()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
importance.ranger()
#| echo: true
rf_test_results %>%
extract_fit_parsnip() |>
importance()
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
importance()
#| echo: true
rf_test_results |>
importance()
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
vip(num_features = 10)
install.packages("vip")
library(vip)
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
vip(num_features = 10)
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification") |>
set_engine("ranger", importance = "impurity")
#Trains top configuration on all training set; predict on test set
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_split_strat)
#| echo: true
#Estiamte unbiased performance on test set
rf_test_results %>% collect_metrics()
# Compare predicted risk to actual outcome
preds <- predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
dt_pred_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pred_outcome,5)
#| echo: true
rf_test_results |>
extract_fit_parsnip() |>
vip(num_features = 10)
?calibration
#| echo: true
#Create grid of hyperparameters for tuning
rf_grid <- grid_regular(
mtry(range = c(1, 10)),#number of predictors sampled at each split of tree
min_n(range = c(2, 11)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
?autoplot
#| echo: true
# install.packages("tidyverse","tidymodels", "ranger")
library(tidyverse)
library(tidymodels)
library(ranger) #random forest engine
theme_set(theme_bw())
#| echo: true
dt <- read_csv("fetal_health.csv") |>
mutate(fetal_health = as.factor(ifelse(fetal_health==1,"Normal", "Abnormal")))
str(dt)
#specify a recipe (what are we predicting, what are the features)
ctg_recipe <- recipe(fetal_health~., data=dt)
#| echo: true
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
set.seed(456)
#Split whole data into test and training, stratifying on the outcome
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Create cross validation folds in test set
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#specify random forest model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune(),
mode = "classification")
rf_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
set.seed(456)
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = rf_grid
)
autoplot(rf_tune_results)
show_best(rf_tune_results)
#| echo: true
#Split whole data into test and training, stratifying on the outcome
set.seed(456)
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Create cross validation folds in test set
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#Split whole data into test and training, stratifying on the outcome
set.seed(456)
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health)
#Create cross validation folds in test set
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#specify random forest model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune(),
mode = "classification")
rf_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = rf_grid
)
autoplot(rf_tune_results)
show_best(rf_tune_results)
#specify a recipe for all individual variables AND pairwise interaction terms
ctg_recipe_interactions <- recipe(fetal_health~.^2, data=dt)
#specify a recipe (what are we predicting, what are the features)
ctg_recipe <- recipe(fetal_health~., data=dt)
#specify a recipe for all individual variables AND pairwise interaction terms
ctg_recipe_interactions <- ctg_recipe |>
step_interact(terms = ~.:.)
ctg_recipe_interactions
#specify elasticnet model for tuning
en_tune_spec <- logistic_reg(penalty = tune(),
mixture = tune(),
mode = "classification")
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
set.seed(456)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe_interactions,
resamples = ctg_folds,
grid = en_grid
)
#specify elasticnet model for tuning
en_tune_spec <- logistic_reg(penalty = tune(),
mixture = tune(),
mode = "classification",
engine = "glmnet")
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
set.seed(456)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe_interactions,
resamples = ctg_folds,
grid = en_grid
)
library(glmnet)
install.packages("glmnet")
library(glmnet)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe_interactions,
resamples = ctg_folds,
grid = en_grid
)
en_tune_results
autoplot(en_tune_results)
?selections
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = en_grid
)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = en_grid
)
autoplot(en_tune_results)
show_best(en_tune_results)
#specify random forest model for tuning
gbm_tune_spec <- boost_tree(mtry = tune(),
trees = 1000,
min_n = tune(),
tree_depth = tune(),
stop_iter = 3,
mode = "classification")
gbm_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),
tree_depth = c(3, 9),
levels = 3
)
gbm_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),
tree_depth(range = c(3, 9)),
levels = 3
)
gbm_grid
set.seed(456)
gbm_tune_results <- tune_grid(
gbm_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = gbm_grid
)
install.packages("xgboost")
gbm_tune_results <- tune_grid(
gbm_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = gbm_grid
)
autoplot(rf_tune_results)
autoplot(gbm_tune_results)
show_best(gbm_tune_results, metric = "roc_auc")
#| echo: true
show_best(rf_tune_results)
show_best(en_tune_results)
show_best(gbm_tune_results)
2*3*3*3
## Load R packages
# install.packages("tidyverse","tidymodels", "ranger")
library(tidyverse)
library(tidymodels) #modeling and machine learning using tidyverse principles
library(ranger) #random forest engine
library(glmnet) #elasticnet engine
library(xgboost) #boosted trees engine
library(flextable) #print tables
library(factoextra) #
theme_set(theme_bw())
#| echo: true
dt <- read.csv("fetal_health.csv") |>
mutate(fetal_health = as.factor(ifelse(fetal_health==1,"Normal", "Abnormal")))
str(dt)
summary(dt$fetal_health)
#specify a recipe (what are we predicting, what are the features)
ctg_recipe <- recipe(fetal_health~., data=dt)
#| echo: true
#Split whole data into test and training, stratifying on the outcome
set.seed(456)
ctg_split_strat <- initial_split(dt, prop = 0.8 ,strata = fetal_health) #creates a single binary split of the data into a training set and testing set
#Create cross validation folds in test set
set.seed(456)
ctg_folds <- vfold_cv(training(ctg_split_strat),
v=3, #three folds
strata = fetal_health, #stratified on outcome
repeats = 2) #two repeats
#| echo: true
#specify random forest model for tuning
rf_tune_spec <- rand_forest(mtry = tune(),
trees = 1000,
min_n = tune(),
mode = "classification")
rf_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),#Minimum datapoints in node for further split
levels = 3
)
rf_grid
# computes a set of performance metrics for a pre-defined set of tuning parameters
set.seed(456)
rf_tune_results <- tune_grid(
rf_tune_spec, # model specification
ctg_recipe, # model formula or recipe
resamples = ctg_folds, # splitted data
grid = rf_grid # tuning combinations
)
autoplot(rf_tune_results) # plot performance metrics
show_best(rf_tune_results, metric = "roc_auc") # display the top sub-models based on AUC
#| echo: true
#specify elasticnet model for tuning
en_tune_spec <- logistic_reg(penalty = tune(),
mixture = tune(),
mode = "classification",
engine = "glmnet")
en_grid <- expand_grid(
mixture = c(0, .33, .66, 1), #1 = lasso model, 0=ridge model
penalty = c(0.1, 0.01, 0.001) #size of penalty
)
en_grid
set.seed(456)
en_tune_results <- tune_grid(
en_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = en_grid
)
autoplot(en_tune_results)
show_best(en_tune_results, metric = "roc_auc")
#| echo: true
#specify random forest model for tuning
gbm_tune_spec <- boost_tree(mtry = tune(),
trees = 1000,
min_n = tune(),
tree_depth = tune(),
stop_iter = 3,
mode = "classification")
gbm_grid <- grid_regular(
mtry(range = c(5, 15)),#number of predictors sampled at each split of tree
min_n(range = c(2, 8)),
tree_depth(range = c(3, 9)),
levels = 3
)
gbm_grid
set.seed(456)
gbm_tune_results <- tune_grid(
gbm_tune_spec,
ctg_recipe,
resamples = ctg_folds,
grid = gbm_grid
)
autoplot(gbm_tune_results)
show_best(gbm_tune_results, metric = "roc_auc")
#| echo: true
show_best(rf_tune_results)
show_best(en_tune_results)
show_best(gbm_tune_results)
#| echo: true
best_auc <- select_best(rf_tune_results, metric = "roc_auc")
#Specify a model with best hyperparameters
rf_best_spec <- rand_forest(mtry = best_auc$mtry,
trees = 1000,
min_n = best_auc$min_n,
mode = "classification") |>
set_engine("ranger", importance = "impurity")
#Trains top configuration on all training set; predict on test set
rf_test_results <- last_fit(
rf_best_spec,
ctg_recipe,
split = ctg_split_strat)
#| echo: true
#Estiamte unbiased performance on test set
rf_test_results %>% collect_metrics()
# Compare predicted risk to actual outcome
preds <- predict(extract_workflow(rf_test_results), testing(ctg_split_strat), type="prob")
dt_pred_outcome <- cbind(preds,
truth =testing(ctg_split_strat)$fetal_health)
head(dt_pred_outcome,5)
roc <- roc_curve(dt_pred_outcome,
truth,
.pred_Abnormal)
head(roc, 5)
autoplot(roc)
## import necessary data
gene_expressions <- read.csv("sc_gene_expression.csv",
header = T, row.names = 1)
head(gene_expressions)|>
flextable()
cells <- read.csv("data_cells.csv")
cells <- read.csv("data_cells.csv")
#create plot of number of clusters vs total within sum of squares
fviz_nbclust(gene_expressions, kmeans, method = "wss") # "wss": total within sum of square
### Perform K-means
set.seed(0)
km.res <- kmeans(gene_expressions, centers = 5)
print(km.res)
### Heatmaps of average gene expression
heatmap(km.res$centers)
### Compare cluster results with known information
#### Sex
prop.table(table(cells$donor_sex_label, km.res$cluster), margin = 2)
#### Region
table_region <- table(cells$region_label, km.res$cluster)
heatmap(table_region, Colv=NA, Rowv=NA)
#### Layer
heatmap(table(cells$layer_label, km.res$cluster), Colv=NA, Rowv=NA)
#### Class
heatmap(table(cells$class_label, km.res$cluster), Colv=NA, Rowv=NA)
